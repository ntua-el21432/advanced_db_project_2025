{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a02568-51f2-4920-8274-fea7262a45b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1114</td><td>application_1765289937462_1107</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1107/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-48.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1107_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c014ccc2f874e769731ae686b2cd548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf6ba83e2fd4bd197c1709d04a00cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0344 with description: Removes vict property has total count: 1002900\n",
      "1822 with description: Stranger has total count: 548422\n",
      "0416 with description: Hit-Hit w/ weapon has total count: 404773\n",
      "0329 with description: Vandalized has total count: 377536\n",
      "0913 with description: Victim knew Suspect has total count: 278618\n",
      "2000 with description: Domestic violence has total count: 256188\n",
      "1300 with description: Vehicle involved has total count: 219082\n",
      "0400 with description: Force used has total count: 213165\n",
      "1402 with description: Evidence Booked (any crime) has total count: 177470\n",
      "1609 with description: Smashed has total count: 131229\n",
      "1309 with description: Susp uses vehicle has total count: 122108\n",
      "1202 with description: Victim was aged (60 & over) or blind/physically disabled/unable to care for self has total count: 120238\n",
      "0325 with description: Took merchandise has total count: 120159\n",
      "1814 with description: Susp is/was current/former boyfriend/girlfriend has total count: 118073\n",
      "0444 with description: Pushed has total count: 116763\n",
      "1501 with description: Other MO (see rpt) has total count: 115589\n",
      "1307 with description: Breaks window has total count: 113609\n",
      "0334 with description: Brandishes weapon has total count: 105665\n",
      "2004 with description: Suspect is homeless/transient has total count:  93426\n",
      "0432 with description: Intimidation has total count:  83562\n",
      "RDD implementation time: 49.28 seconds"
     ]
    }
   ],
   "source": [
    "#  1: rdd api implementation\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"RDD query 1 execution\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .getOrCreate() \\\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "crime1_df= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\", \\\n",
    "        header=True, inferSchema=True)\n",
    "crime2_df= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\", \\\n",
    "        header=True, inferSchema=True)\n",
    "crime_df= crime1_df.union(crime2_df)\n",
    "\n",
    "mocodes_rdd = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_codes.txt\")\n",
    "crime_data_rdd = crime_df.rdd\n",
    "\n",
    "# take mocode , description from mocodes.txt\n",
    "mocodes_map = mocodes_rdd.map(lambda x: (x.split(\" \", 1)[0], x.split(\" \", 1)[1]))\n",
    "\n",
    "mocodes_counts = (\n",
    "    crime_data_rdd\n",
    "    .flatMap(lambda x: x[10].split() if x[10] is not None else [])     # split each string like \"0913 1814 2000\"\n",
    "    .filter(lambda c: c.strip() != \"\")    # remove empty ones\n",
    "    .map(lambda c: (c.strip(), 1))        # (mocode, 1)\n",
    "    .reduceByKey(lambda a, b: a + b)      # count occurrences\n",
    ")\n",
    "\n",
    "joined = mocodes_counts.join(mocodes_map)\n",
    "sorted_mocodes = joined.sortBy(lambda x: x[1][0], ascending=False)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "for code, (count, desc) in sorted_mocodes.take(20):\n",
    "    print(f\"{code} with description: {desc} has total count: {count:>6}\")\n",
    "print(f\"RDD implementation time: {end_time-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f530257-787f-4403-b8a8-ef0a886886f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1123</td><td>application_1765289937462_1116</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1116/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-251.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1116_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0224557b878c4a17bac4c50a42def7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f95be334dc403fa3f5b50e777a6684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0344 with description: Removes vict property has total count: 1002900\n",
      "1822 with description: Stranger has total count: 548422\n",
      "0416 with description: Hit-Hit w/ weapon has total count: 404773\n",
      "0329 with description: Vandalized has total count: 377536\n",
      "0913 with description: Victim knew Suspect has total count: 278618\n",
      "2000 with description: Domestic violence has total count: 256188\n",
      "1300 with description: Vehicle involved has total count: 219082\n",
      "0400 with description: Force used has total count: 213165\n",
      "1402 with description: Evidence Booked (any crime) has total count: 177470\n",
      "1609 with description: Smashed has total count: 131229\n",
      "1309 with description: Susp uses vehicle has total count: 122108\n",
      "1202 with description: Victim was aged (60 & over) or blind/physically disabled/unable to care for self has total count: 120238\n",
      "0325 with description: Took merchandise has total count: 120159\n",
      "1814 with description: Susp is/was current/former boyfriend/girlfriend has total count: 118073\n",
      "0444 with description: Pushed has total count: 116763\n",
      "1501 with description: Other MO (see rpt) has total count: 115589\n",
      "1307 with description: Breaks window has total count: 113609\n",
      "0334 with description: Brandishes weapon has total count: 105665\n",
      "2004 with description: Suspect is homeless/transient has total count:  93426\n",
      "0432 with description: Intimidation has total count:  83562\n",
      "\n",
      "Dataframe implementation time with broadcast join: 24.77 seconds"
     ]
    }
   ],
   "source": [
    "# 2: Dataframe implementation\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, explode, trim, col\n",
    "\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"dataframe query 2 execution\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .getOrCreate() \\\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "crime1_df= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\", \\\n",
    "        header=True, inferSchema=True)\n",
    "crime2_df= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\", \\\n",
    "        header=True, inferSchema=True)\n",
    "crime_df= crime1_df.union(crime2_df)\n",
    "\n",
    "mocodes_txt_df = spark.read.text(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_codes.txt\"\n",
    ")\n",
    "# split into (code, description)\n",
    "mocodes_df = mocodes_txt_df.select(\n",
    "    col(\"value\").substr(1, 4).alias(\"mocode\"),\n",
    "    trim(col(\"value\").substr(6, 1000)).alias(\"description\")\n",
    ")\n",
    "mocode_counts_df = (\n",
    "    crime_df\n",
    "    .select(explode(split(col(\"Mocodes\"), \" \")).alias(\"mocode\"))\n",
    "    .filter(col(\"mocode\") != \"\")     # drop empty codes\n",
    "    .groupBy(\"mocode\")\n",
    "    .count()\n",
    ")\n",
    "# comment out the following lines if you want to test the rest of the implementations\n",
    "# joined_df , choice = mocode_counts_df.join(mocodes_df, on=\"mocode\", how=\"inner\"),0\n",
    "# joined_df.explain(True)\n",
    "\n",
    "broadcast_join , choice = mocode_counts_df.join(mocodes_df.hint(\"BROADCAST\"),\"mocode\") ,1\n",
    "# merge_join , choice = mocode_counts_df.hint(\"MERGE\").join(mocodes_df, \"mocode\") , 2\n",
    "# sh_hash_join , choice = mocode_counts_df.hint(\"SHUFFLE_HASH\").join(mocodes_df, \"mocode\") ,3 \n",
    "# repnl_join , choice = mocode_counts_df.hint(\"SHUFFLE_REPLICATE_NL\").join(mocodes_df, \"mocode\") ,4\n",
    "\n",
    "# function to give the joined df from the implementation we want \n",
    "# uncomment the one we want each time and pass it to the chosen_df variable\n",
    "chosen_df = broadcast_join\n",
    "def collect_func(df):\n",
    "    sorted_df = (df.orderBy(col(\"count\").desc()))\n",
    "    for row in sorted_df.limit(20).collect():\n",
    "        print(f\"{row['mocode']} with description: {row['description']} has total count: {row['count']:>6}\")\n",
    "\n",
    "collect_func(chosen_df)\n",
    "end_time = time.time()\n",
    "if choice==0:\n",
    "    print(f\"\\nDataframe implementation time with catalyst optimizer: {end_time-start_time:.2f} seconds\")\n",
    "if choice==1:\n",
    "    print(f\"\\nDataframe implementation time with broadcast join: {end_time-start_time:.2f} seconds\")\n",
    "if choice==2:\n",
    "    print(f\"\\nDataframe implementation time with merge join: {end_time-start_time:.2f} seconds\")\n",
    "if choice==3:\n",
    "    print(f\"\\nDataframe implementation time with shuffe hash join: {end_time-start_time:.2f} seconds\")\n",
    "if choice==4:\n",
    "    print(f\"\\nDataframe implementation time with shuffle replicate join: {end_time-start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a2e5b25-05c6-4325-b272-b171a99747a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb51f1b750fe4798847ae92662e7fd68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcast: 1.6267642974853516\n",
      "Merge:     1.6859371662139893\n",
      "ShuffleHash: 1.4366343021392822\n",
      "ShuffleReplicateNL: 1.9512789249420166"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "\n",
    "# def compare_join_time(df,starting_time):\n",
    "#     df.count()\n",
    "#     return time.time() - starting_time\n",
    "\n",
    "# broadcast_join = mocode_counts_df.join(mocodes_df.hint(\"BROADCAST\"), \"mocode\")\n",
    "# merge_join = mocode_counts_df.hint(\"MERGE\").join(mocodes_df, \"mocode\")\n",
    "# sh_hash_join = mocode_counts_df.hint(\"SHUFFLE_HASH\").join(mocodes_df, \"mocode\")\n",
    "# repnl_join = mocode_counts_df.hint(\"SHUFFLE_REPLICATE_NL\").join(mocodes_df, \"mocode\")\n",
    "\n",
    "# print(\"Broadcast:\", compare_join_time(broadcast_join))\n",
    "# print(\"Merge:    \", compare_join_time(merge_join))\n",
    "# print(\"ShuffleHash:\", compare_join_time(sh_hash_join))\n",
    "# print(\"ShuffleReplicateNL:\", compare_join_time(repnl_join))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e78d4-8149-45bb-b1b2-ab47c0cd244d",
   "metadata": {},
   "source": [
    "Παρατηρούμε ότι τον καλύτερο χρόνο κάνει το Shuffle hash join. Δεύτερο καλύτερο χρόνο κάνει το broadcast join. Αυτό συμβαίνει μάλλον αφού το shuffle hash join αποφεύγει την ταξινόμηση των δεδομένων ενώ το broadcast έχει χαμηλότερη απόδοση λόγο του broadcast του πίνακα mo_codes προς τους executors. Το sort merge join επιβαρύνεται από το sort ενώ το shuffle replicate nested loop πιθανώς δεν αρμόζει σε τόσο μεγάλα δεδομένα."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
