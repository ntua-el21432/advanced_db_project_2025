{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b69115-1002-4f57-a8a2-ae88f9346d62",
   "metadata": {},
   "source": [
    "Ζητούμενο 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a022747-fb66-410a-b2a9-e2774f88eda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1675</td><td>application_1765289937462_1659</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1659/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-154.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1659_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18cccc8284e64f908fd240eb4be631dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3877e2dd2e461c9cef20cd3e26446a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adults: 121660\n",
      "Young Adults: 33758\n",
      "Children: 16014\n",
      "Seniors: 6011\n",
      "\n",
      "RDD implementation time: 33.77 seconds"
     ]
    }
   ],
   "source": [
    "#  1: rdd api implementation\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"RDD query 1 execution\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .getOrCreate() \\\n",
    "\n",
    "sc = spark.sparkContext\n",
    "import time \n",
    "\n",
    "def find_age_group(age):\n",
    "    try:\n",
    "        age = int(age)\n",
    "    except:\n",
    "        return None\n",
    "    if(age<18):\n",
    "        return \"Children\"\n",
    "    if(age<25):\n",
    "        return \"Young Adults\"\n",
    "    if(age<65):\n",
    "        return \"Adults\"\n",
    "    return \"Seniors\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "crime_df1 = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\",header=False)\n",
    "crime_df2 = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",header=False)\n",
    "crime_df = crime_df1.union(crime_df2)\n",
    "\n",
    "crime_rdd = crime_df.rdd\n",
    "\n",
    "sorted_age_rdd = crime_rdd.filter(lambda x: \"aggravated assault\" in x[9].lower()).map(lambda x: (find_age_group(x[11]),1)) \\\n",
    "    .reduceByKey(lambda x,y: x+y).sortBy(lambda value: value[1], ascending=False)\n",
    "\n",
    "results = sorted_age_rdd.collect()\n",
    "end_time = time.time()\n",
    "\n",
    "for group,count in sorted_age_rdd.collect():\n",
    "    print(f\"{group}: {count}\")\n",
    "print(f\"\\nRDD implementation time: {end_time-start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda9d558-8822-4570-b368-c95f93c6ed56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1745</td><td>application_1765289937462_1729</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1729/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1729_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bce51a274a449a828d14a50a057d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af456971af54a2d95ae8a0c1e0b208a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|Age Group   |count |\n",
      "+------------+------+\n",
      "|Adults      |121660|\n",
      "|Young Adults|33758 |\n",
      "|Children    |16014 |\n",
      "|Elders      |6011  |\n",
      "+------------+------+\n",
      "\n",
      "\n",
      "DataFrame (no UDF) Implementation time: 18.37 seconds"
     ]
    }
   ],
   "source": [
    "# Implementation 2: DataFrame API ( no UDF )\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import col, lower , when\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Dataframe (no UDF) query 1 execution\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "crime_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType(), True),\n",
    "    StructField(\"Date_Rptd\", StringType(), True),\n",
    "    StructField(\"DATE_OCC\", StringType(), True),\n",
    "    StructField(\"TIME_OCC\", StringType(), True),\n",
    "    StructField(\"AREA\", StringType(), True),\n",
    "    StructField(\"AREA_NAME\", StringType(), True),\n",
    "    StructField(\"Rpt_Dist_No\", StringType(), True),\n",
    "    StructField(\"Part_1_2\", StringType(), True),\n",
    "    StructField(\"Crm_Cd\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_Desc\", StringType(), True),\n",
    "    StructField(\"Mocodes\", StringType(), True),\n",
    "    StructField(\"Vict Age\", StringType(), True),\n",
    "])\n",
    "\n",
    "crime1_df= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\", \\\n",
    "        header=False, schema= crime_schema)\n",
    "crime2_df= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\", \\\n",
    "        header=False, schema= crime_schema)\n",
    "crime_df= crime1_df.union(crime2_df)\n",
    "\n",
    "assault_df = crime_df.filter(lower(col(\"Crm_Cd_Desc\")).contains(\"aggravated assault\"))\n",
    "assault_grouped_df = assault_df.withColumn(\n",
    "    \"Age Group\",\n",
    "    when(col(\"Vict Age\") < 18, \"Children\")\n",
    "    .when((col(\"Vict Age\") >= 18) & (col(\"Vict Age\") <= 24), \"Young Adults\")\n",
    "    .when((col(\"Vict Age\") >= 25) & (col(\"Vict Age\") <= 64), \"Adults\")\n",
    "    .when(col(\"Vict Age\") > 64, \"Elders\")\n",
    "    .otherwise(None)  \n",
    ")\n",
    "age_group_counts = assault_grouped_df.groupBy(\"Age Group\").count().orderBy(col(\"count\").desc())\n",
    "age_group_counts.show(truncate=False)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nDataFrame (no UDF) Implementation time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d1b1a50-fb4c-4d19-ac6e-c276df79eac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1682</td><td>application_1765289937462_1666</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1666/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-141.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1666_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc05dc46880f46bca9d9126c2454522d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f625742df0452583b1ce696f5909c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|Age Group      |count |\n",
      "+---------------+------+\n",
      "|????????       |121660|\n",
      "|?????? ????????|33758 |\n",
      "|??????         |16014 |\n",
      "|??????????     |6011  |\n",
      "+---------------+------+\n",
      "\n",
      "\n",
      "Dataframe (with UDF) implementation time: 23.44 seconds"
     ]
    }
   ],
   "source": [
    "# Implementation 3: DataFrame API ( with UDF )\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import col, lower, udf\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Dataframe (with UDF) query 1 execution\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "import time\n",
    "\n",
    "def find_age_group(age):\n",
    "    try:\n",
    "        age = int(age)\n",
    "    except:\n",
    "        return None\n",
    "    if(age<18):\n",
    "        return \"Παιδιά\"\n",
    "    if(age<25):\n",
    "        return \"Νεαροί ενήλικοι\"\n",
    "    if(age<65):\n",
    "        return \"Ενήλικοι\"\n",
    "    return \"Ηλικωμένοι\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "crime_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType(), True),\n",
    "    StructField(\"Date_Rptd\", StringType(), True),\n",
    "    StructField(\"DATE_OCC\", StringType(), True),\n",
    "    StructField(\"TIME_OCC\", StringType(), True),\n",
    "    StructField(\"AREA\", StringType(), True),\n",
    "    StructField(\"AREA_NAME\", StringType(), True),\n",
    "    StructField(\"Rpt_Dist_No\", StringType(), True),\n",
    "    StructField(\"Part_1_2\", StringType(), True),\n",
    "    StructField(\"Crm_Cd\", StringType(), True),\n",
    "    StructField(\"Crm_Cd_Desc\", StringType(), True),\n",
    "    StructField(\"Mocodes\", StringType(), True),\n",
    "    StructField(\"Vict Age\", StringType(), True),\n",
    "])\n",
    "\n",
    "crime1_df= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\", \\\n",
    "        header=False, schema= crime_schema)\n",
    "crime2_df= spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\", \\\n",
    "        header=False, schema= crime_schema)\n",
    "crime_df= crime1_df.union(crime2_df)\n",
    "\n",
    "aggr_df = crime_df.filter(lower(col(\"Crm_Cd_Desc\")).contains(\"aggravated assault\"))\n",
    "\n",
    "find_age_group_udf = udf(find_age_group, StringType())\n",
    "\n",
    "aggr_grouped_df = aggr_df.withColumn(\"Age Group\",find_age_group_udf(col(\"Vict Age\")))\n",
    "group_counts = aggr_grouped_df.groupBy(\"Age Group\").count().orderBy(col(\"count\").desc())\n",
    "group_counts.show(truncate=False)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nDataframe (with UDF) implementation time: {end_time -start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
